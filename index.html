<!DOCTYPE HTML>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Guian Fang (ÊñπÊ°ÇÂÆâ)</title>
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
    <link rel="stylesheet" href="stylesheet.css">
    <script src="https://unpkg.com/feather-icons"></script>
</head>
<body>
 <!-- Âú∞ÂõæÈÉ®ÂàÜ -->
<!-- Â∑¶‰æßÂú∞Âõæ -->
<a href='https://clustrmaps.com/site/1c0ri' title='Visit tracker' style="position: fixed; top: 0; left: 2vw; z-index: 999;">
    <img src='//clustrmaps.com/map_v2.png?cl=080808&w=a&t=tt&d=OQrAUhFj6mJpqpj_tmlLF3TJxCsBZJ1Y9W4766QT7XU&co=ffffff&ct=808080' alt='ClustrMaps Tracker' style="width: 10vw; min-width: 120px; max-width: 180px;"/>
</a>

<!-- Âè≥‰æßcitation map -->
<a href="citation_map.html" style="position: fixed; top: 0; right: 2vw; z-index: 999;">
    <img src="citation_map.png" alt="Citation Map" style="width: 10vw; min-width: 120px; max-width: 180px;"/>
</a>

<!-- ÂØºËà™Ê†èÈÉ®ÂàÜ -->
<nav style="padding: 2 0vw;"> <!-- ‰ΩøÁî®vwÂçï‰Ωç‰ΩøpaddingÈöèËßÜÁ™óÂÆΩÂ∫¶ÂèòÂåñ -->
    <ul>
        <li><a href="#research" id="nav-research">üîç Research</a></li>
        <li><a href="#honors" id="nav-honors">üèÜ Honors & Awards</a></li>
        <li><a href="#activities" id="nav-activities">üåü Activities & Services</a></li>
        <li><a href="#acknowledgements" id="nav-acknowledgements">üìú Acknowledgements</a></li>
    </ul>
    <a href="#top" class="to-top" style="display:none;">‚¨ÜÔ∏è</a>
    <div class="social-icons">
        <a href="mailto:guianfang@u.nus.edu"><i class="fas fa-envelope"></i></a>
        <a href="data/cv.pdf"><i class="fas fa-file-alt"></i></a>
        <a href="https://scholar.google.com/citations?hl=en&user=2z4sraUAAAAJ"><i class="fas fa-graduation-cap"></i></a>
        <a href="https://www.linkedin.com/in/enderfga" target="_blank"><i class="fab fa-linkedin"></i></a>
        <a href="https://twitter.com/Enderfga" target="_blank"><i class="fab fa-twitter"></i></a>
        <a href="https://github.com/Enderfga" target="_blank"><i class="fab fa-github"></i></a>
        <div class="theme-switcher">
            <span><i data-feather="sun"></i></span>
            <div class="three-way-radio">
                <input type="radio" name="ui-mode" id="footer-ui-mode-left" class="left" value="left" data-action="ui-mode-light">
                <input type="radio" name="ui-mode" id="footer-ui-mode-center" class="center" value="center" data-action="ui-mode-system">
                <input type="radio" name="ui-mode" id="footer-ui-mode-right" class="right" value="right" data-action="ui-mode-dark">
                <div class="indicator"></div>
            </div>
            <span><i data-feather="moon"></i></span>
        </div>
    </div>
</nav>
    <div id="content">
        <section class="profile-section">
            <div class="profile-text">
                <p class="name">Guian Fang</p>
                <p>I am a Ph.D. student at <a href="https://sites.google.com/view/showlab/">Show Lab</a>, <strong><a href="https://www.nus.edu.sg/">National University of Singapore</a></strong>, advised by <a href="https://scholar.google.com/citations?user=h1-3lSoAAAAJ&hl">Prof. Mike Zheng Shou</a>.</p>
                <p>Previously, I received my B.Eng. in HCPLab, Artificial Intelligence at the School of Intelligent Systems Engineering, <strong><a href="https://ise.sysu.edu.cn/">Sun Yat-sen University</a></strong>, advised by Xiaodan Liang <a href="https://scholar.google.com/citations?view_op=search_authors&mauthors=xiaodan+liang&hl=zh-CN&oi=ao">(Ê¢ÅÂ∞è‰∏π)</a>, co-supervised by <a href="https://shengcailiao.github.io/">Shengcai Liao</a>.</p>
                <p>Additionally, I was a research scientist at <strong><a href="https://www.cybever.ai/">Cybever AI</a></strong> in Sunnyvale, where I cooperated with a small team that mostly works on 3D computer vision and Large language model.</p>
            </div>
            <div class="profile-image" onmouseover="guian_start()" onmouseout="guian_stop()" style="position: relative; display: inline-block;">
                <div class="image-container" id="guian_image" style="opacity: 0; position: absolute; top: 0; left: 0; width: 100%; height: 100%;">
                  <img src="images/guian.gif" alt="profile photo" style="width: 100%; height: 100%; object-fit: cover;">
                </div>
                <a href="images/guian.jpg">
                  <img src="images/guian.jpg" alt="profile photo" style="max-width: 100%; height: auto; display: block;">
                </a>
              </div>
        </section>


        <section id="research">
            <h2>Research</h2>
            <p>I'm interested in computer vision, deep learning, generative AI, and AI alignment. Most of my research will be about video understanding and generation. Representative papers are <span class="highlight">highlighted</span>.</p>
            <table style="width:100%;">
                <tr>
                    <td style="padding:10px;vertical-align:middle">
                        <div style="position:relative; padding-bottom:56.25%; height:0; overflow:hidden;">
                            <iframe title="vimeo-player" src="https://player.vimeo.com/video/985036345?h=c1f9f7c34b" style="position:absolute; top:0; left:0; width:100%; height:100%; border:none;" frameborder="0"    allowfullscreen></iframe>
                        </div>
                    </td>
                </tr>
                <tr>
                    <td style="padding:15px;vertical-align:middle">
                        <a href="https://www.cybever.ai/">
                            <span class="papertitle">Generate 3D Worlds in Production with AI</span>
                        </a>
                        <br>
                        Cybever*, <strong>Guian Fang</strong>*
                        <br>
                        <p>
                            Image-to-3D: Let AI do the heavy lifting so 3D Professionals can do the storytelling    
                        </p>
                    </td>
                </tr>
            </table>
            
            <table bgcolor="#ffffd0" style="width:100%;">
                <tr>
                  <td style="padding:10px;vertical-align:middle">
                    <img src='images/humanrefiner.jpg' style="width:100%; margin:auto; display:block;">
                  </td>
                </tr>
                <tr>
                  <td style="padding:15px;vertical-align:middle">
                    <a href="https://github.com/Enderfga/HumanRefiner">
                      <span class="papertitle">HumanRefiner: Benchmarking Abnormal Human Generation and Refining with Coarse-to-fine Pose-Reversible Guidance</span>
                    </a>
                    <br>
                    <strong>Guian Fang</strong>*, Wenbiao Yan*, Yuanfan Guo*, Jianhua Han, Zutao Jiang, Hang Xu, Shengcai Liao, Xiaodan Liang
                    <br>
                    <em>ECCV</em>, 2024
                    <br>
                    <a href="https://huggingface.co/datasets/Enderfga/HumanRefiner">Dataset page</a> /
                    <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/04696.pdf">Paper</a>
                    <p>
                      In this project, we introduce AbHuman, the first large-scale benchmark focused on anatomical anomalies. The benchmark consists of 56K synthesized human images, each annotated with 147K human anomalies in 18 different categories. Based on this, we developed HumanRefiner, a novel plug-and-play method for coarse-to-fine refinement of human anomalies.
                    </p>
                  </td>
                </tr>
              </table>
              
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tr>
                  <td style="width:25%; padding:20px; vertical-align:middle">
                    <img src='images/ChartThinker.png' width="330">
                  </td>
                  <td style="width:75%; padding:20px; vertical-align:middle">
                    <a href="https://github.com/Notonion/ChartThinker">
                      <span class="papertitle">ChartThinker: A Contextual Chain-of-Thought Approach to Optimized Chart Summarization</span>
                    </a>
                    <br>
                    Mengsha Liu, Daoyuan Chen, Yaliang Li, <strong>Guian Fang</strong>, Ying Shen
                    <br>
                    <em>LREC-Coling</em>, 2024
                    <br>
                    <a href="https://huggingface.co/datasets/ChartThinker/Chart-Sum-QA">Dataset page</a> /
                    <a href="https://aclanthology.org/2024.lrec-main.273/">Paper</a>
                    <p>
                      In this project, we address the challenges of chart summarization through our development of ChartThinker, an innovative method that leverages natural language processing to enhance visual-language matching and reasoning capabilities. Our approach involves a large-scale dataset featuring diverse chart-caption pairs and detailed fine-tuning instructions, significantly improving training data effectiveness. ChartThinker employs thought chains and context retrieval strategies to produce logically coherent and accurate summaries. Demonstrating superior performance, our model outperforms eight state-of-the-art models across seven evaluation metrics.
                    </p>
                  </td>
                </tr>
              </table>

              <table bgcolor="#ffffd0" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tr>
                  <td style="width:25%; padding:20px; vertical-align:middle">
                    <img src='images/llama2.png' width="330">
                  </td>
                  <td style="width:75%; padding:20px; vertical-align:middle">
                    <a href="https://github.com/Alpha-VLLM/LLaMA2-Accessory">
                      <span class="papertitle">LLaMA2-Accessory: An Open-source Toolkit for LLM Development</span>
                    </a>
                    <br>
                    Chris Liu, Ziyi Lin, <strong>Guian Fang</strong>, Jiaming Han, Yijiang Liu, Renrui Zhang, Longtian Qiu, Yichi Zhang, Siyuan Huang
                    <br>
                    <img src="https://img.shields.io/github/stars/Alpha-VLLM/LLaMA2-Accessory?style=social&amp;label=Star" alt="Star">
                    <br>
                    <a href="https://huggingface.co/Alpha-VLLM/LLaMA2-Accessory">Checkpoint page</a> /
                    <a href="https://llama2-accessory.readthedocs.io/en/latest/">Document</a>
                    <p>
                        LLaMA2-Accessory is an open-source toolkit for pretraining, finetuning and deployment of Large Language Models (LLMs) and multimodal LLMs. This repo is mainly inherited from <a href="https://github.com/OpenGVLab/LLaMA-Adapter">LLaMA-Adapter</a> with more advanced features.</p>
                  </td>
                </tr>
              </table>
              
        </section>
        <section id="honors">
            <h2>Honors & Awards</h2>
            <ul>
                <li>2nd Place, Asia and Pacific Mathematical Contest in Modeling (2022)</li>
                <li>Silver Medal, China Collegiate Algorithm Design & Programming Challenge Contest (2022)</li>
                <li>2nd Place, Social Computing Innovation Competition (2022)</li>
                <li>1st Place, National College Computer Ability Challenge (2023)</li>
                <li>Recipient, Huawei Intelligent Foundation Scholarship (2022)</li>
                <li>Recipient, National Encouragement scholarship (2022)</li>
                <li>1st Place, SYSU Outstanding Student Scholarship (2023)</li>
                <li>Recipient, National Scholarship (2023)</li>
                <li>Recipient, Li Xuerou Foundation Scholarship (2023)</li>
            </ul>
        </section>
        <section id="activities">
            <h2>Activities & Services</h2>
            <ul>
                <li>Reviewer for ECCV</li>
                <li>Reviewer for NeurIPS</li>
		<li>Reviewer for ICLR</li>
		<li>Reviewer for ICML</li>    
		<li>Reviewer for AISTATS</li>
                <li>Core organizer of <a href="https://sites.google.com/view/loveucvpr24/home">LOVEU: LOng-form VidEo Understanding Towards Multimodal AI Assistant and Copilot Workshop @ CVPR'24</a>
                </li>
            </ul>
        </section>
        <section id="acknowledgements">
            <h2>Acknowledgements</h2>
           <p>I feel incredibly fortunate to have collaborated with such remarkable individuals who have generously offered me their mentorship.</p>
		
        	<table style="width:100%; border:0; border-spacing:0; border-collapse:separate; margin-right:auto; margin-left:auto;">
    <tbody>
        <tr>
            <td style="padding:1%; width:33%; vertical-align:top; text-align:center;">
                <img style="height:100px;" src="images/cybever_logo.jpeg" class="hoverZoomLink" alt="Cybever Logo">
                <p>
                    <a href="https://www.cybever.ai/">@ Cybever Inc.</a>
                </p>
                <p>
                    <a href="https://www.linkedin.com/in/cnjieyang/">CTO, Jie Yang</a>
                </p>
            </td>
            <td style="padding:1%; width:33%; vertical-align:top; text-align:center;">
                <img style="height:100px;" src="images/gvlab.jpeg" class="hoverZoomLink" alt="GVLab Logo">
                <p>
                    <a href="https://alpha-vllm.github.io/">@ OpenGVLab, Shanghai AI Lab</a>
                </p>
                <p>
                    <a href="https://gaopengcuhk.github.io/">Dr. Peng Gao</a>
                </p>
            </td>
            <td style="padding:1%; width:33%; vertical-align:top; text-align:center;">
                <img style="height:100px;" src="images/huawei.jpg" class="hoverZoomLink" alt="Huawei Logo">
                <p>
                    <a href="http://dev3.noahlab.com.hk/index.html">@ Noah's Ark Lab, Huawei</a>
                </p>
                <p>
                    <a href="https://xuhangcn.github.io/">Dr. Hang Xu</a>
                </p>
            </td>
        </tr>
    </tbody>
</table>
        <footer>
            <p>Feel free to steal this website's <a href="https://github.com/Enderfga/enderfga.github.io">source code</a>. <strong>Do not</strong> scrape the HTML from this page itself.</p>
        </footer>
    </div>
    <script>
        // Á™ÅÂá∫ÊòæÁ§∫ÂΩìÂâçÂØºËà™È°π
        document.querySelectorAll('nav a').forEach(link => {
            link.addEventListener('click', function() {
                document.querySelectorAll('nav a').forEach(nav => nav.style.textDecoration = 'none');
                this.style.textDecoration = 'underline';
            });
        });

        // ÊòæÁ§∫ÊàñÈöêËóèÂõûÂà∞È°∂ÈÉ®ÊåâÈíÆ
        window.onscroll = function() {
            if (document.body.scrollTop > 20 || document.documentElement.scrollTop > 20) {
                document.querySelector('.to-top').style.display = "block";
            } else {
                document.querySelector('.to-top').style.display = "none";
            }
        };

        document.addEventListener('DOMContentLoaded', (event) => {
            feather.replace();

            const body = document.body;
            const radios = document.querySelectorAll('.three-way-radio input');

            radios.forEach(radio => {
                radio.addEventListener('change', () => {
                    if (radio.classList.contains('left')) {
                        body.classList.remove('dark-mode');
                    } else if (radio.classList.contains('right')) {
                        body.classList.add('dark-mode');
                    } else {
                        // Implement system default mode logic here
                    }
                });
            });

            // Click the icons to change mode
            document.querySelector('.theme-switcher span:first-child').addEventListener('click', () => {
                document.getElementById('footer-ui-mode-left').checked = true;
                body.classList.remove('dark-mode');
            });

            document.querySelector('.theme-switcher span:last-child').addEventListener('click', () => {
                document.getElementById('footer-ui-mode-right').checked = true;
                body.classList.add('dark-mode');
            });

            // Initialize to light mode by default
            document.getElementById('footer-ui-mode-left').checked = true;
        });
    </script>
    <script type="text/javascript">
        function guian_start() {
          document.getElementById('guian_image').style.opacity = "1";
        }
      
        function guian_stop() {
          document.getElementById('guian_image').style.opacity = "0";
        }
        guian_stop();
      </script>
</body>
</html>
